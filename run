import argparse
import numpy as np
from copy import deepcopy
from xuance.common import get_configs, recursive_dict_update
from xuance.torch.utils.operations import set_seed
from xuance.torch.agents import MASAC_Agents
##########################
##########################
"这套训练代码是借鉴了玄策官方给的MASAC算法的训练代码，包括测试、基准测试、训练等功能，因为没有对于奖励值的直接反应，而是给出了一个得分，因此催生出了run_xuance_2"
##########################
##########################
# 将字典转换为 Namespace 对象的辅助函数（假设 configs_dict 是一个字典）
def dict_to_namespace(d):
    if isinstance(d, dict):
        return argparse.Namespace(**{k: dict_to_namespace(v) for k, v in d.items()})
    return d

# 解析命令行参数
def parse_args():
    parser = argparse.ArgumentParser("Example of XuanCe: MASAC for MPE.")
    parser.add_argument("--env-id", type=str, default="env_xuance")
    parser.add_argument("--test", type=int, default=0)     # 测试
    parser.add_argument("--benchmark", type=int, default=0)  # 训练
    return parser.parse_args()

if __name__ == "__main__":
    # 解析命令行参数
    parser = parse_args()

    # 加载配置文件
    configs_dict = get_configs(file_dir="MASAC_config.yaml")
    configs_dict = recursive_dict_update(configs_dict, parser.__dict__)
    configs = dict_to_namespace(configs_dict)

    # 设置随机种子
    set_seed(configs.seed)

    # 导入自定义环境
    from env_xuance_1 import MicrogridEnvContinuous
    from xuance.environment import REGISTRY_MULTI_AGENT_ENV

    # 注册自定义环境
    REGISTRY_MULTI_AGENT_ENV[configs.env_name] = MicrogridEnvContinuous

    # 创建并初始化环境
    from xuance.environment import make_envs
    envs = make_envs(configs)

    # 创建MASAC智能体
    Agent = MASAC_Agents(config=configs, envs=envs)

    # 打印训练信息
    train_information = {
        "Deep learning toolbox": configs.dl_toolbox,
        "Calculating device": configs.device,
        "Algorithm": configs.agent,
        "Environment": configs.env_name,
        "Scenario": configs.env_id
    }
    for k, v in train_information.items():
        print(f"{k}: {v}")

    # 如果开启了基准测试
    if configs.benchmark:
        # 定义用于测试环境的函数
        def env_fn():
            configs_test = deepcopy(configs)
            configs_test.parallels = configs_test.test_episode
            return make_envs(configs_test)

        train_steps = configs.running_steps // configs.parallels
        eval_interval = configs.eval_interval // configs.parallels
        test_episode = configs.test_episode
        num_epoch = int(train_steps / eval_interval)

        # 进行初步的测试
        test_scores = Agent.test(env_fn, test_episode)
        Agent.save_model(model_name="best_model.pth")
        best_scores_info = {
            "mean": np.mean(test_scores),
            "std": np.std(test_scores),
            "step": Agent.current_step
        }

        # 训练过程中的每个epoch
        for i_epoch in range(num_epoch):
            print(f"Epoch: {i_epoch}/{num_epoch}:")
            # 训练指定的步数
            Agent.train(eval_interval)
            # 测试并获取分数
            test_scores = Agent.test(env_fn, test_episode)

            # 如果当前的平均得分比之前更好，保存模型
            if np.mean(test_scores) > best_scores_info["mean"]:
                best_scores_info = {
                    "mean": np.mean(test_scores),
                    "std": np.std(test_scores),
                    "step": Agent.current_step
                }
                Agent.save_model(model_name="best_model.pth")
        # 输出最佳模型的测试结果
        print(f"Best Model Score: {best_scores_info['mean']:.2f}, std={best_scores_info['std']:.2f}")

    else:
        # 如果是测试模式
        if configs.test:
            def env_fn():
                configs.parallels = configs.test_episode
                return make_envs(configs)

            # 加载已经训练好的模型
            Agent.load_model(path=Agent.model_dir_load)
            # 进行测试
            scores = Agent.test(env_fn, configs.test_episode)
            print(f"Mean Score: {np.mean(scores)}, Std: {np.std(scores)}")
            print("Finish testing.")
        else:
            # 启动训练过程
            Agent.train(configs.running_steps // configs.parallels)
            # 保存最终模型
            Agent.save_model("final_train_model.pth")
            print("Finish training!")

    # 完成训练
    Agent.finish()
